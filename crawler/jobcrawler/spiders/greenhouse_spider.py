import scrapy, orjson, datetime as dt\nfrom dateutil.parser import isoparse\n\nclass GreenhouseSpider(scrapy.Spider):\n    name = \"greenhouse\"\n    custom_settings = {\"DOWNLOAD_DELAY\": 0.5}\n\n    def start_requests(self):\n        companies = [\"openai\",\"databricks\",\"anthropic\"]  # seed; move to config\n        for c in companies:\n            url = f\"https://boards-api.greenhouse.io/v1/boards/{c}/jobs\"\n            yield scrapy.Request(url, callback=self.parse_jobs, cb_kwargs={\"company\": c})\n\n    def parse_jobs(self, resp, company):\n        data = orjson.loads(resp.text)\n        now = dt.datetime.now(dt.timezone.utc)\n        for j in data.get(\"jobs\", []):\n            posted_at = isoparse(j.get(\"updated_at\") or j.get(\"created_at\"))\n            if (now - posted_at).total_seconds() > 24*3600:\n                continue\n            yield {\n                \"source\": \"Greenhouse\",\n                \"company\": j.get(\"offices\", [{}])[0].get(\"name\", company),\n                \"title\": j[\"title\"],\n                \"location\": (j.get(\"location\") or {}).get(\"name\"),\n                \"remote\": \"remote\" if \"remote\" in j[\"title\"].lower() else None,\n                \"employment_type\": None,\n                \"level\": None,\n                \"posted_at\": posted_at.isoformat(),\n                \"apply_url\": j.get(\"absolute_url\"),\n                \"canonical_url\": j.get(\"absolute_url\"),\n                \"currency\": None,\n                \"salary_min\": None,\n                \"salary_max\": None,\n                \"salary_period\": None,\n                \"description_md\": scrapy.selector.Selector(text=j.get(\"content\",\"\")) .xpath(\"string(.)\").get(),\n                \"description_raw\": j.get(\"content\"),\n                \"meta\": j,\n            }\n